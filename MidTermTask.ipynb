{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcxSzJpUgly5"
      },
      "source": [
        "Raissa Anggia Maharani\n",
        "\n",
        "2206048581"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmTNSaZ8gpsk"
      },
      "source": [
        "For this Hands-On Prompts, I choose YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBbEyHvfgy0u"
      },
      "source": [
        "1. In Google Colab, install the libraries necessary for real-time object detection using YOLO and video processing using OpenCV. Explain the roles of each library\n",
        "Install the libraries required for YOLO object detection and OpenCV for handling video input and output. Include PyTorch for running the YOLO model, and ensure that GPU acceleration is enabled in Google Colab to enhance performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2lof1K0gZfU"
      },
      "outputs": [],
      "source": [
        "# Install PyTorch with GPU support\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Install OpenCV for video processing\n",
        "!pip install opencv-python-headless\n",
        "\n",
        "# Install YOLOv5 from the official repository (includes dependencies like PyTorch, PyYAML, etc.)\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1y-8g7qh4o8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Should return True if GPU is enabled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JD2K86Tiiv4"
      },
      "source": [
        "2. Since Google Colab doesn't support direct webcam input, upload a video file and load the pre-trained YOLOv5 model for object detection\n",
        "Explain how to upload a video file and mount Google Drive in Colab. Load the YOLOv5 model for detecting objects in the video frames. Discuss the architecture of YOLO and how it processes each frame, detecting multiple objects and outputting bounding boxes and class labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6b0GBVfiwS3"
      },
      "source": [
        "Upload Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewwbtTY0ilPP"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload the video file (e.g., .mp4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGOB_PdQkcin"
      },
      "source": [
        "Load YOLOv5 for Object Detection\n",
        "\n",
        "YOLOv5 comes with pre-trained models that you can load easily. You can either use a small, fast model like yolov5s or a larger, more accurate model like yolov5x. Hereâ€™s how to load the pre-trained model and perform object detection on the video frames:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQQyIrQEO-Kz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow  # Import cv2_imshow\n",
        "\n",
        "# Load the YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "# Load video\n",
        "cap = cv2.VideoCapture('/content/race_car.mp4')\n",
        "\n",
        "# Loop through video frames\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Perform object detection on the frame\n",
        "    results = model(frame)\n",
        "\n",
        "    # Get results\n",
        "    results.render()  # This modifies the frame with bounding boxes and labels\n",
        "\n",
        "    # Display the frame\n",
        "    cv2_imshow(frame)  # Use cv2_imshow instead of cv2.imshow\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6HE5M2vM1vT"
      },
      "source": [
        "3. Detect objects frame by frame using YOLOv5 and extract relevant information (bounding boxes, class labels)\n",
        "Process each frame using YOLOv5, extracting bounding boxes and object classifications. Discuss how YOLO handles multiple detections in real time, and how to structure the extracted data (bounding boxes, labels) for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaSuRQ51mGHB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "# Load the YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "# Load video\n",
        "video_path = '/content/race_car.mp4'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Prepare a list to store the extracted information\n",
        "extracted_data = []\n",
        "\n",
        "# Loop through video frames\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Perform object detection on the frame\n",
        "    results = model(frame)\n",
        "\n",
        "    # Extract bounding boxes, class labels, and confidences\n",
        "    for detection in results.xyxy[0]:  # [x1, y1, x2, y2, confidence, class]\n",
        "        x1, y1, x2, y2, conf, cls = detection.tolist()\n",
        "        label = model.names[int(cls)]\n",
        "\n",
        "        # Store the relevant data for this frame\n",
        "        extracted_data.append({\n",
        "            'frame_id': cap.get(cv2.CAP_PROP_POS_FRAMES),  # Current frame number\n",
        "            'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2,\n",
        "            'confidence': conf,\n",
        "            'class_id': int(cls),\n",
        "            'class_label': label\n",
        "        })\n",
        "\n",
        "    # (Optional) Display the frame with bounding boxes\n",
        "    results.render()  # This modifies the frame with bounding boxes and labels\n",
        "    cv2.imshow('YOLOv5 Object Detection', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release video capture and destroy windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# Convert extracted data into a DataFrame for analysis\n",
        "df = pd.DataFrame(extracted_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAkGJ0IsrdIu"
      },
      "source": [
        "Saving Extracted Data to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQd-6WF4rfOF"
      },
      "outputs": [],
      "source": [
        "df.to_csv('yolo_detections.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCQWDxAUq4Yv"
      },
      "source": [
        "Normalize the Bounding Box Coordinates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU6pb6aQq5AV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "\n",
        "# Load the YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "# Load the video\n",
        "video_path = '/content/race_car.mp4'  # Replace with your video path\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get frame width and height for normalization\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Prepare a list to store the extracted information\n",
        "extracted_data = []\n",
        "\n",
        "# Loop through video frames\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Perform object detection on the current frame\n",
        "    results = model(frame)  # Pass the frame to the YOLO model\n",
        "\n",
        "    # Extract bounding boxes, class labels, and confidence scores from results\n",
        "    for detection in results.xyxy[0]:  # [x1, y1, x2, y2, confidence, class]\n",
        "        x1, y1, x2, y2, conf, cls = detection.tolist()\n",
        "\n",
        "        # Normalize the bounding box coordinates\n",
        "        x1_norm = x1 / frame_width\n",
        "        y1_norm = y1 / frame_height\n",
        "        x2_norm = x2 / frame_width\n",
        "        y2_norm = y2 / frame_height\n",
        "\n",
        "        # Store the relevant data for this frame\n",
        "        extracted_data.append({\n",
        "            'frame_id': int(cap.get(cv2.CAP_PROP_POS_FRAMES)),  # Current frame number\n",
        "            'x1_norm': x1_norm, 'y1_norm': y1_norm, 'x2_norm': x2_norm, 'y2_norm': y2_norm,\n",
        "            'confidence': conf,\n",
        "            'class_id': int(cls),\n",
        "            'class_label': model.names[int(cls)]  # Convert class index to label\n",
        "        })\n",
        "\n",
        "# Release video capture after processing\n",
        "cap.release()\n",
        "\n",
        "# Now, the 'extracted_data' list contains all the detection data for further use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5m2-Wqcq-C4"
      },
      "source": [
        "4. Preprocess the object detection results and store them for further analysis\n",
        "Explain how to preprocess the YOLO output by normalizing coordinates and converting the data into a structured format suitable for tracking and further analysis. Highlight the importance of maintaining consistent object identifiers across frames to facilitate effective tracking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2DYFKUdq-7X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "# Load the YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "# Load video\n",
        "video_path = '/content/race_car.mp4'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get frame width and height for normalization\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Prepare a list to store the extracted information\n",
        "extracted_data = []\n",
        "\n",
        "# Loop through video frames\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Perform object detection on the frame\n",
        "    results = model(frame)\n",
        "\n",
        "    # Normalize and extract bounding boxes, class labels, and confidences\n",
        "    for detection in results.xyxy[0]:  # [x1, y1, x2, y2, confidence, class]\n",
        "        x1, y1, x2, y2, conf, cls = detection.tolist()\n",
        "\n",
        "        # Normalize the bounding box coordinates\n",
        "        x1_norm = x1 / frame_width\n",
        "        y1_norm = y1 / frame_height\n",
        "        x2_norm = x2 / frame_width\n",
        "        y2_norm = y2 / frame_height\n",
        "\n",
        "        # Store the relevant data for this frame\n",
        "        extracted_data.append({\n",
        "            'frame_id': int(cap.get(cv2.CAP_PROP_POS_FRAMES)),  # Current frame number\n",
        "            'x1_norm': x1_norm, 'y1_norm': y1_norm, 'x2_norm': x2_norm, 'y2_norm': y2_norm,\n",
        "            'confidence': conf,\n",
        "            'class_id': int(cls),\n",
        "            'class_label': model.names[int(cls)]\n",
        "        })\n",
        "\n",
        "# Release video capture\n",
        "cap.release()\n",
        "\n",
        "# Convert the extracted data into a DataFrame for analysis\n",
        "df = pd.DataFrame(extracted_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi4KoWI1rJ-P"
      },
      "source": [
        " Saving Preprocessed Data for Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr71VH3jrG3D"
      },
      "outputs": [],
      "source": [
        " # Save preprocessed data to a CSV file for future use\n",
        "df.to_csv('preprocessed_yolo_detections.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mMEc_fxNJ0Y"
      },
      "source": [
        "5. Implement object tracking using YOLOv5 output and explain the significance of tracking objects over multiple frames\n",
        "Track objects across frames, focusing on the need to maintain identity consistency. This ensures that objects can be followed and tracked accurately as they move across multiple frames. Discuss various object tracking methods and their importance in real-time video applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUUo3Mfgugol"
      },
      "source": [
        "Install DeepSORT and YOLOv5 Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD0GmxWsuibW"
      },
      "outputs": [],
      "source": [
        "!pip install deep-sort-realtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF_LMpGfukoL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "\n",
        "# Load YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "# Initialize DeepSORT tracker\n",
        "tracker = DeepSort(max_age=30, nn_budget=70, nms_max_overlap=1.0)\n",
        "\n",
        "# Load video\n",
        "video_path = '/content/race_car.mp4'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Loop through video frames\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Perform object detection with YOLOv5\n",
        "    results = model(frame)\n",
        "\n",
        "    # Extract detection results (xyxy format)\n",
        "    dets = []\n",
        "    for detection in results.xyxy[0]:  # [x1, y1, x2, y2, confidence, class]\n",
        "        x1, y1, x2, y2, conf, cls = detection.tolist()\n",
        "        # Convert to format [x1, y1, width, height] for DeepSORT\n",
        "        bbox = [x1, y1, x2 - x1, y2 - y1]\n",
        "        dets.append((bbox, conf, int(cls)))\n",
        "\n",
        "    # Update tracker with the current frame's detections\n",
        "    tracks = tracker.update_tracks(dets, frame=frame)\n",
        "\n",
        "    # Draw bounding boxes and track IDs on frame\n",
        "    for track in tracks:\n",
        "        if not track.is_confirmed():\n",
        "            continue\n",
        "\n",
        "        track_id = track.track_id\n",
        "        ltrb = track.to_ltrb()  # Get left, top, right, bottom coordinates\n",
        "        class_id = track.get_class()\n",
        "\n",
        "        # Draw bounding box and track ID\n",
        "        cv2.rectangle(frame, (int(ltrb[0]), int(ltrb[1])), (int(ltrb[2]), int(ltrb[3])), (255, 0, 0), 2)\n",
        "        cv2.putText(frame, f'ID: {track_id} Class: {class_id}', (int(ltrb[0]), int(ltrb[1]) - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    # Display the frame (optional)\n",
        "    cv2.imshow('Tracked Frame', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XSZUJbAv_xL"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "\n",
        "# Load the YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "# Open the video file\n",
        "video_path = '/race_car.mp4'  # Ganti dengan path ke video Anda\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get video properties\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Define the codec and create VideoWriter object\n",
        "output_path = '/race_car.mp4'  # Ganti dengan path untuk menyimpan video output\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec untuk mp4\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Perform inference on the frame\n",
        "    results = model(frame)\n",
        "\n",
        "    # Process results\n",
        "    # results.xyxy[0] contains bounding box coordinates and confidence scores\n",
        "    for *xyxy, conf, cls in results.xyxy[0]:\n",
        "        label = f'{model.names[int(cls)]} {conf:.2f}'  # Get label and confidence\n",
        "        # Draw bounding box on the frame\n",
        "        cv2.rectangle(frame, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (255, 0, 0), 2)\n",
        "        cv2.putText(frame, label, (int(xyxy[0]), int(xyxy[1] - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "    # Write the frame with detections to the output video\n",
        "    out.write(frame)\n",
        "\n",
        "    # Optionally display the resulting frame\n",
        "    cv2.imshow('YOLOv5 Object Detection', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54QHhpdUxWXo"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "\n",
        "# Load the YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "# Open the video file\n",
        "video_path = '/content/race_car.mp4'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get video properties\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Define the codec and create VideoWriter object\n",
        "output_path = '/content/output_video.mp4'\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"No more frames to read.\")\n",
        "        break\n",
        "\n",
        "    # Perform inference on the frame\n",
        "    results = model(frame)\n",
        "\n",
        "    # Process results\n",
        "    for *xyxy, conf, cls in results.xyxy[0]:\n",
        "        label = f'{model.names[int(cls)]} {conf:.2f}'\n",
        "        cv2.rectangle(frame, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (255, 0, 0), 2)\n",
        "        cv2.putText(frame, label, (int(xyxy[0]), int(xyxy[1] - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "    # Write the frame with detections to the output video\n",
        "    out.write(frame)\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "out.release()\n",
        "print(\"Finished processing video.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxJfTMcTxgiw"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Video\n",
        "\n",
        "# Menampilkan video yang telah disimpan\n",
        "Video('/content/output_video.mp4', embed=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d0DnFZiNVjo"
      },
      "source": [
        "5. Optimize YOLO performance in Google Colab by adjusting input parameters and enabling GPU acceleration\n",
        "Explore strategies to optimize the YOLOv5 pipeline for real-time performance. This can include adjusting the input resolution to balance speed and accuracy, skipping frames to process videos faster, and using model optimizations like quantization to reduce inference time while maintaining detection quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLtQAbegFxqC"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # Clone YOLOv5 repository\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt  # Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU8CawNaFzZQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Load YOLOv5 model with lower input size for faster processing\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)  # Load YOLOv5 small model\n",
        "model.eval()  # Set model to evaluation mode\n",
        "model.stride = 32  # Stride size for downsampling\n",
        "model.img_size = 416  # Reduce input size to 416x416 for faster processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTf-GzZMGDNA"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Function to process video with skipping frames\n",
        "def process_video_with_skipping(input_video_path, output_video_path, skip_frames=3):\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Define video output\n",
        "    out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps // skip_frames, (frame_width, frame_height))\n",
        "\n",
        "    frame_count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Only process every 'skip_frames' frames\n",
        "        if frame_count % skip_frames == 0:\n",
        "            # Run YOLOv5 inference on frame\n",
        "            results = model(frame)\n",
        "            results.render()  # Draw bounding boxes on frame\n",
        "\n",
        "            # Write processed frame to output video\n",
        "            out.write(results.imgs[0])\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(\"Finished processing video.\")\n",
        "\n",
        "# Example usage\n",
        "process_video_with_skipping('input_video.mp4', 'output_video.mp4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRKQkBgHGGsV"
      },
      "outputs": [],
      "source": [
        "from torch.quantization import quantize_dynamic\n",
        "import cv2\n",
        "\n",
        "# Perform quantization on YOLOv5 model\n",
        "quantized_model = quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8  # Quantize linear layers to int8\n",
        ")\n",
        "\n",
        "# Function to run quantized model on each frame of a video\n",
        "def run_quantized_inference_on_video(input_video_path, output_video_path):\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Define video output settings\n",
        "    out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Run inference on the current frame\n",
        "        results = quantized_model(frame)  # Process frame with quantized model\n",
        "        results.render()  # Draw bounding boxes and labels on the frame\n",
        "\n",
        "        # Write the processed frame to the output video\n",
        "        out.write(results.imgs[0])\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(\"Finished processing video with quantized model.\")\n",
        "\n",
        "# Example usage\n",
        "run_quantized_inference_on_video('input_video.mp4', 'output_video_quantized.mp4')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}